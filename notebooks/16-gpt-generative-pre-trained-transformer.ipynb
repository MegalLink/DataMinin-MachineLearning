{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-22T21:24:59.127666Z","iopub.execute_input":"2023-08-22T21:24:59.128040Z","iopub.status.idle":"2023-08-22T21:24:59.158283Z","shell.execute_reply.started":"2023-08-22T21:24:59.128010Z","shell.execute_reply":"2023-08-22T21:24:59.157359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"GPT is a model and approach developed by OpenAI. It is primarily known for its capabilities in generating coherent and contextually relevant text over long passages.\n\nArchitecture: GPT is based on the Transformer architecture. Unlike some other models that use both an encoder and a decoder, GPT exclusively utilizes the decoder part of the Transformer for its tasks.\n\nPre-training and Fine-tuning:\n\nPre-training: GPT is first pre-trained on a large corpus of text (like books, articles, websites, etc.). During this phase, it learns to predict the next word in a sentence. This process enables the model to learn grammar, facts about the world, reasoning abilities, and even some level of common sense.\nFine-tuning: After pre-training, the model can be fine-tuned on a specific task, such as translation, question-answering, or summarization, using a smaller, task-specific dataset.\nAutoregressive Nature: GPT generates text in an autoregressive manner. This means it produces one word at a time and uses what it's generated so far as a context to generate the next word.\n\nKey Features:\nGenerative Abilities: As the name suggests, GPT excels at generating text. It can produce text that is often indistinguishable from what a human might write.\nFew-Shot Learning: Introduced with GPT-3, this capability allows the model to perform tasks even when provided with very few examples (sometimes as few as one). By just specifying a task in natural language, GPT-3 can often understand and perform the task without explicit fine-tuning.\nVersatility: Unlike many models that are trained for a specific task, GPT models, especially GPT-3, are versatile and can handle a wide range of tasks without task-specific training. This includes writing essays, answering questions, creating poetry, generating code, and much more.\nVersions:\nGPT: The original model introduced by OpenAI.\n\nGPT-2: A larger and more powerful version that garnered significant attention due to its impressive text generation capabilities. OpenAI initially withheld the fully-trained model due to concerns about misuse, but later released it given the broader community's responsible usage.\n\nGPT-3: The third iteration with 175 billion parameters, making it one of the largest models ever created. It introduced the concept of few-shot and zero-shot learning, further advancing the state-of-the-art in various NLP tasks.\n\nExercise: Exploring Creative Writing with GPT\nYour task is to use OpenAI's GPT model to generate creative content. You'll explore various prompts and settings to see how GPT responds and creates different outputs.\n\nLoad GPT-2 Model and Tokenizer. GPT-2 is freely available in Hugging Face's model hub and is still highly effective.","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\nmodel_name = 'gpt2-medium'  # You can start with 'gpt2' (smaller) and then experiment with larger models\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2023-08-22T21:24:59.160479Z","iopub.execute_input":"2023-08-22T21:24:59.161006Z","iopub.status.idle":"2023-08-22T21:25:38.597260Z","shell.execute_reply.started":"2023-08-22T21:24:59.160966Z","shell.execute_reply":"2023-08-22T21:25:38.595948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generate Creative Content Function","metadata":{}},{"cell_type":"code","source":"def generate_creative_content(prompt, max_length=150, temperature=1.0):\n    \"\"\"Generate creative content using GPT-2 based on a given prompt.\"\"\"\n\n    # Encode the prompt\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n\n    # Generate text\n    output = model.generate(input_ids, max_length=max_length, temperature=temperature, pad_token_id=tokenizer.eos_token_id)\n\n    # Decode and print the generated text\n    generated_text = tokenizer.decode(output[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n\n    return generated_text","metadata":{"execution":{"iopub.status.busy":"2023-08-22T21:25:38.598699Z","iopub.execute_input":"2023-08-22T21:25:38.599669Z","iopub.status.idle":"2023-08-22T21:25:38.605963Z","shell.execute_reply.started":"2023-08-22T21:25:38.599628Z","shell.execute_reply":"2023-08-22T21:25:38.604736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Experiment:\nUse various prompts and observe GPT's creative capabilities. Change parameters like max_length and temperature to see their impact. (Note: A higher temperature value makes output more random, while a lower value makes it more deterministic.)","metadata":{}},{"cell_type":"code","source":"prompts = [\n    \"Once upon a time, in a kingdom far away,\",\n    \"In a dystopian future, where AI rules the world,\",\n    \"The last dinosaur on Earth was not like the others. It\",\n    \"Deep beneath the ocean waves, a secret civilization\"\n]\n\nfor prompt in prompts:\n    print(f\"Prompt: {prompt}\")\n    print(generate_creative_content(prompt))\n    print(\"\\n\" + \"-\"*50 + \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-08-22T21:25:38.607860Z","iopub.execute_input":"2023-08-22T21:25:38.608206Z","iopub.status.idle":"2023-08-22T21:26:30.541813Z","shell.execute_reply.started":"2023-08-22T21:25:38.608177Z","shell.execute_reply":"2023-08-22T21:26:30.541052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Discussion and Analysis:\n\nAnalyze the quality of the generated text: coherence, relevancy, and creativity.\nDiscuss how different prompts influence the direction of the story.\nExperiment with custom prompts to generate different genres of creative content (e.g., horror, sci-fi, romance).\nTasks:\n","metadata":{}},{"cell_type":"code","source":"El texto comienza bien pero despues pierder coherecia y repite lo mismo varias veces","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompts = [\n    \"In a creepy rousty house\",\n    \"Software development career\",\n]\n\nfor prompt in prompts:\n    print(f\"Prompt: {prompt}\")\n    print(generate_creative_content(prompt))\n    print(\"\\n\" + \"-\"*50 + \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-08-22T21:34:10.966457Z","iopub.execute_input":"2023-08-22T21:34:10.966845Z","iopub.status.idle":"2023-08-22T21:34:37.828314Z","shell.execute_reply.started":"2023-08-22T21:34:10.966813Z","shell.execute_reply":"2023-08-22T21:34:37.827193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nUse a larger GPT-2 variant (gpt2-large or gpt2-xl) and compare the quality of outputs.\nIncorporate user feedback loops, where after getting an initial piece of text, they can provide a follow-up prompt to continue or steer the story.","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\nmodel_name = 'gpt2-large'  # You can also use 'gpt2-xl'\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\n\ndef generate_creative_content(prompt, max_length=150, temperature=1.0):\n    \"\"\"Generate creative content using GPT-2 based on a given prompt.\"\"\"\n\n    # Encode the prompt\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n\n    # Generate text\n    output = model.generate(input_ids, max_length=max_length, temperature=temperature, pad_token_id=tokenizer.eos_token_id)\n\n    # Decode and print the generated text\n    generated_text = tokenizer.decode(output[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n\n    return generated_text\n","metadata":{"execution":{"iopub.status.busy":"2023-08-22T21:36:25.175842Z","iopub.execute_input":"2023-08-22T21:36:25.176251Z","iopub.status.idle":"2023-08-22T21:37:25.961786Z","shell.execute_reply.started":"2023-08-22T21:36:25.176216Z","shell.execute_reply":"2023-08-22T21:37:25.960431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def user_feedback_loop(prompt):\n    generated_text = generate_creative_content(prompt)\n   \n    while True:\n        # Get user feedback\n        print(prompt)\n        print(generated_text)\n        feedback = input(\"What do you think of this? (Y/N/S) \")\n\n        if feedback == \"Y\":\n            # Continue generating text\n            prompt = generated_text\n            generated_text = generate_creative_content(prompt)\n\n        elif feedback == \"N\":\n            # Start over with a new prompt\n            prompt = input(\"Give me a new prompt: \")\n            generated_text = generate_creative_content(prompt)\n\n        elif feedback == \"S\":\n            # Stop the loop\n            break\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-22T21:41:19.036245Z","iopub.execute_input":"2023-08-22T21:41:19.036686Z","iopub.status.idle":"2023-08-22T21:41:19.044125Z","shell.execute_reply.started":"2023-08-22T21:41:19.036647Z","shell.execute_reply":"2023-08-22T21:41:19.042902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"In a creepy rousty house\"\n\nuser_feedback_loop(prompt)","metadata":{"execution":{"iopub.status.busy":"2023-08-22T21:41:23.799388Z","iopub.execute_input":"2023-08-22T21:41:23.800210Z","iopub.status.idle":"2023-08-22T21:43:29.073844Z","shell.execute_reply.started":"2023-08-22T21:41:23.800157Z","shell.execute_reply":"2023-08-22T21:43:29.072828Z"},"trusted":true},"execution_count":null,"outputs":[]}]}