{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Transformers are a type of deep learning architecture that have revolutionized the field of natural language processing (NLP) due to their remarkable performance on a variety of tasks.\n\nKey Characteristics:\n\nAttention Mechanisms: At the heart of Transformers is the self-attention mechanism that can weigh input tokens differently, allowing the model to focus on various parts of the input data. This mechanism can capture relationships between tokens regardless of their positions or distance from one another.\n\nParallel Processing: Unlike recurrent neural networks (RNNs) that process data sequentially, Transformers process all tokens in the input data in parallel, which leads to significant speed-ups during training.\n\nScalability: Transformers are highly scalable. This means they can be trained with a vast number of parameters (often billions), leading to models like GPT-3 from OpenAI.\n\nPositional Encoding: Since Transformers don't process data sequentially, they don't have a built-in notion of the order or position of tokens. To address this, positional encodings are added to the embeddings at the input layer, providing the model with positional context.\n\nTransformer Architecture\nThe Transformer model consists of an Encoder-Decoder structure. Both the encoder and the decoder are composed of a stack of identical layers.\n\nEncoder: The encoder receives the input data (like a sentence) and compresses the information into a 'context' or 'memory' that the decoder can then use. The encoder consists of a stack of identical layers. Each layer has two main components: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.\nInput Embedding: The input data is first converted into vectors using embedding layers. Positional encoding is then added to these embeddings to give the model information about the position of words in a sequence.\n\nEncoder Stack: Multiple (often 6 or more) identical layers are stacked. Each layer has two main components: (1) Multi-Head Attention Mechanism: This allows the encoder to focus on different parts of the input sentence when producing the context. It uses the attention mechanism we discussed earlier but multiple times in parallel. (2) Feed-Forward Neural Network: Each attention output is then passed through a feed-forward neural network (the same one for each position).\n\nDecoder: The decoder generates the output data (like the translation of the input sentence) from the context provided by the encoder. Also consists of a stack of identical layers. In addition to the two components in the encoder layer, the decoder has a third component, which is a multi-head attention over the encoder's output.\nOutput Embedding: Like the input embedding but for the output data.\n\nDecoder Stack: Also composed of multiple identical layers. Each layer has three main components:\n\n(1) Masked Multi-Head Attention Mechanism: This ensures that the prediction for a particular word doesn’t depend on future words in the sequence. It's \"masked\" to prevent the model from \"cheating\" by looking ahead. (2) Multi-Head Attention Mechanism (over encoder’s output): This helps the decoder focus on relevant parts of the input sentence, much like in the encoder but attending to the encoder's output. (3) Feed-Forward Neural Network: Just like in the encoder.\n\nThe final layer of the decoder produces the output sequence.","metadata":{}},{"cell_type":"code","source":"from transformers import MarianMTModel, MarianTokenizer\n\nmodel_name = 'Helsinki-NLP/opus-mt-en-es'\nmodel = MarianMTModel.from_pretrained(model_name)\ntokenizer = MarianTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2023-08-22T21:21:02.668930Z","iopub.execute_input":"2023-08-22T21:21:02.669343Z","iopub.status.idle":"2023-08-22T21:21:26.181022Z","shell.execute_reply.started":"2023-08-22T21:21:02.669308Z","shell.execute_reply":"2023-08-22T21:21:26.179681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = model.get_encoder()\nprint(\"Encoder:\", encoder)\ndecoder = model.get_decoder()\nprint(\"Decoder:\", decoder)","metadata":{"execution":{"iopub.status.busy":"2023-08-22T21:21:42.029145Z","iopub.execute_input":"2023-08-22T21:21:42.029593Z","iopub.status.idle":"2023-08-22T21:21:42.038121Z","shell.execute_reply.started":"2023-08-22T21:21:42.029555Z","shell.execute_reply":"2023-08-22T21:21:42.036693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def translate_to_spanish(phrase):\n    \"\"\"Translate an English phrase to Spanish using the pre-trained Transformer model.\"\"\"\n    # Tokenize and encode the phrase\n    encoded_phrase = tokenizer.encode(phrase, return_tensors=\"pt\")  # Use \"tf\" for TensorFlow\n    # Generate the translation from the encoded phrase\n    translation_ids = model.generate(encoded_phrase)\n    # Convert token IDs back to a string\n    translation = tokenizer.decode(translation_ids[0], skip_special_tokens=True)\n    return translation","metadata":{"execution":{"iopub.status.busy":"2023-08-22T21:21:49.053566Z","iopub.execute_input":"2023-08-22T21:21:49.054941Z","iopub.status.idle":"2023-08-22T21:21:49.062099Z","shell.execute_reply.started":"2023-08-22T21:21:49.054897Z","shell.execute_reply":"2023-08-22T21:21:49.060954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(translate_to_spanish(\"the quick brown fox jumps over the lazy dog\"))","metadata":{"execution":{"iopub.status.busy":"2023-08-22T21:21:54.111333Z","iopub.execute_input":"2023-08-22T21:21:54.111734Z","iopub.status.idle":"2023-08-22T21:21:54.651833Z","shell.execute_reply.started":"2023-08-22T21:21:54.111702Z","shell.execute_reply":"2023-08-22T21:21:54.650658Z"},"trusted":true},"execution_count":null,"outputs":[]}]}