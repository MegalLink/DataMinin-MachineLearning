{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-04T03:40:12.079112Z","iopub.execute_input":"2023-08-04T03:40:12.079705Z","iopub.status.idle":"2023-08-04T03:40:12.089340Z","shell.execute_reply.started":"2023-08-04T03:40:12.079652Z","shell.execute_reply":"2023-08-04T03:40:12.087747Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":"11 Recurrent Neural Networks\nRecurrent Neural Networks (RNNs) are a class of artificial neural networks designed to process sequential data, where the order of data points matters. Unlike feedforward neural networks, RNNs have connections that form loops, allowing information to persist and be passed from one step to the next. This capability makes RNNs well-suited for tasks involving time series, natural language processing, speech recognition, video analysis, and more.\n\nBy leveraging their recurrent connections and hidden state, RNNs excel at capturing temporal dependencies in sequential data. However, training RNNs effectively remains a challenge, particularly for long sequences. Various advanced RNN variants, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), have been introduced to address the vanishing and exploding gradient problem and improve the modeling capabilities of RNNs for a wider range of applications.\n\nKey Concepts of RNNs:\nRecurrent Connections:\nThe defining feature of RNNs is the presence of recurrent connections, which allow information to be retained and propagated through time. In each step of the sequence, an RNN processes the current input along with the hidden state from the previous step, updating the hidden state accordingly. This feedback mechanism enables RNNs to learn dependencies and patterns across different time steps. 2. Hidden State: The hidden state of an RNN acts as its memory and captures relevant information from the past. It is continuously updated at each time step and serves as an internal representation of the input sequence. 3. Training Challenges: Training RNNs can be challenging due to the vanishing and exploding gradient problem. During backpropagation through time (BPTT), gradients can either become too small (vanish) or too large (explode), leading to poor convergence or training instability. This phenomenon occurs when the network has to propagate information over long sequences, and it becomes difficult for the gradients to accurately propagate back to the initial time steps.\n\nRNN Architectures:\nThere are different variations of RNN architectures, including Elman RNN, Jordan RNN, and bidirectional RNNs.\n\nElman RNN: In an Elman RNN, the hidden state is fed back to the network's input at the next time step, creating a simple feedback loop. This architecture is suitable for many sequential tasks but can suffer from vanishing gradients for long sequences.\nJordan RNN: In a Jordan RNN, the hidden state is fed back to the network's output at the current time step. This type of architecture can be useful for specific problems but is less commonly used compared to Elman RNNs and other more advanced RNN variants.\nBidirectional RNNs: Bidirectional RNNs process the input sequence in both forward and backward directions, allowing the model to consider future information as well. This is particularly useful for tasks where context from both past and future elements is essential, such as speech recognition and machine translation.","metadata":{}},{"cell_type":"markdown","source":"Exercise\nUse the IMDB movie reviews dataset to perform sentiment analysis with a simple RNN.","metadata":{}},{"cell_type":"code","source":"from keras.datasets import imdb\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, SimpleRNN, Dense, Bidirectional\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2023-08-04T03:40:12.092054Z","iopub.execute_input":"2023-08-04T03:40:12.092545Z","iopub.status.idle":"2023-08-04T03:40:12.104641Z","shell.execute_reply.started":"2023-08-04T03:40:12.092507Z","shell.execute_reply":"2023-08-04T03:40:12.103584Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"max_features = 5000  # Number of words to consider as features\nmax_len_short = 100  # Maximum sequence length for short sequences\nmax_len_long = 500   # Maximum sequence length for long sequences\n\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T03:40:12.106664Z","iopub.execute_input":"2023-08-04T03:40:12.107074Z","iopub.status.idle":"2023-08-04T03:40:18.679281Z","shell.execute_reply.started":"2023-08-04T03:40:12.107038Z","shell.execute_reply":"2023-08-04T03:40:18.677634Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"markdown","source":"Pad sequences to a fixed length for RNN input","metadata":{}},{"cell_type":"code","source":"x_train_short = tf.keras.utils.pad_sequences(x_train, maxlen=max_len_short)\nx_test_short = tf.keras.utils.pad_sequences(x_test, maxlen=max_len_short)\n\nx_train_long = tf.keras.utils.pad_sequences(x_train, maxlen=max_len_long)\nx_test_long =  tf.keras.utils.pad_sequences(x_test, maxlen=max_len_long)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T03:40:18.687052Z","iopub.execute_input":"2023-08-04T03:40:18.687546Z","iopub.status.idle":"2023-08-04T03:40:19.939331Z","shell.execute_reply.started":"2023-08-04T03:40:18.687506Z","shell.execute_reply":"2023-08-04T03:40:19.937956Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"markdown","source":"Build the RNN model","metadata":{}},{"cell_type":"code","source":"def build_rnn_model():\n    model = Sequential()\n    model.add(Embedding(max_features, 32))\n    model.add(SimpleRNN(32, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-08-04T03:40:19.942137Z","iopub.execute_input":"2023-08-04T03:40:19.942792Z","iopub.status.idle":"2023-08-04T03:40:19.949539Z","shell.execute_reply.started":"2023-08-04T03:40:19.942751Z","shell.execute_reply":"2023-08-04T03:40:19.948236Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import LSTM\ndef build_rnn_model_lstm():\n    model = Sequential()\n    model.add(Embedding(max_features, 16))\n    model.add(LSTM(16, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-08-04T03:40:19.951076Z","iopub.execute_input":"2023-08-04T03:40:19.951487Z","iopub.status.idle":"2023-08-04T03:40:19.961319Z","shell.execute_reply.started":"2023-08-04T03:40:19.951452Z","shell.execute_reply":"2023-08-04T03:40:19.960227Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"def build_rnn_model_bi():\n    model = Sequential()\n    model.add(Embedding(max_features, 32))\n    model.add(Bidirectional(SimpleRNN(32, activation='relu')))\n    model.add(Dense(1, activation='sigmoid'))\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-08-04T03:40:19.962671Z","iopub.execute_input":"2023-08-04T03:40:19.963001Z","iopub.status.idle":"2023-08-04T03:40:19.977272Z","shell.execute_reply.started":"2023-08-04T03:40:19.962973Z","shell.execute_reply":"2023-08-04T03:40:19.976288Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"markdown","source":"Train and evaluate the RNN model","metadata":{}},{"cell_type":"code","source":"def train_and_evaluate_model(model, x_train, y_train, x_test, y_test):\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    #model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n    #history = model.fit(x_train, y_train, epochs=5, batch_size=128, validation_split=0.2)\n    history = model.fit(x_train, y_train, epochs=4, batch_size=512)\n    loss, accuracy = model.evaluate(x_test, y_test)\n    return loss, accuracy, history","metadata":{"execution":{"iopub.status.busy":"2023-08-04T03:40:19.978457Z","iopub.execute_input":"2023-08-04T03:40:19.979208Z","iopub.status.idle":"2023-08-04T03:40:19.991249Z","shell.execute_reply.started":"2023-08-04T03:40:19.979173Z","shell.execute_reply":"2023-08-04T03:40:19.989906Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"markdown","source":"Train and evaluate RNN on short and long sequences","metadata":{}},{"cell_type":"code","source":"print(\"\\nTraining SimpleRNN model on short sequences:\")\nrnn_model_short = build_rnn_model_bi()\nloss_short, accuracy_short, history_short = train_and_evaluate_model(\n    rnn_model_short, x_train_short, y_train, x_test_short, y_test\n)\n\nprint(\"\\nTraining SimpleRNN model on long sequences:\")\nrnn_model_long = build_rnn_model_bi()\nloss_long, accuracy_long, history_long = train_and_evaluate_model(\n    rnn_model_long, x_train_long, y_train, x_test_long, y_test\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T03:40:19.992593Z","iopub.execute_input":"2023-08-04T03:40:19.992971Z","iopub.status.idle":"2023-08-04T03:42:16.898572Z","shell.execute_reply.started":"2023-08-04T03:40:19.992938Z","shell.execute_reply":"2023-08-04T03:42:16.897652Z"},"trusted":true},"execution_count":111,"outputs":[{"name":"stdout","text":"\nTraining SimpleRNN model on short sequences:\nEpoch 1/4\n49/49 [==============================] - 5s 59ms/step - loss: 0.6801 - accuracy: 0.5708\nEpoch 2/4\n49/49 [==============================] - 3s 59ms/step - loss: 0.5204 - accuracy: 0.7882\nEpoch 3/4\n49/49 [==============================] - 3s 66ms/step - loss: 0.3557 - accuracy: 0.8549\nEpoch 4/4\n49/49 [==============================] - 3s 60ms/step - loss: 0.2901 - accuracy: 0.8836\n782/782 [==============================] - 8s 9ms/step - loss: 0.3761 - accuracy: 0.8359\n\nTraining SimpleRNN model on long sequences:\nEpoch 1/4\n49/49 [==============================] - 18s 307ms/step - loss: 0.6876 - accuracy: 0.5595\nEpoch 2/4\n49/49 [==============================] - 15s 296ms/step - loss: 0.5498 - accuracy: 0.7631\nEpoch 3/4\n49/49 [==============================] - 15s 301ms/step - loss: 0.3987 - accuracy: 0.8318\nEpoch 4/4\n49/49 [==============================] - 15s 297ms/step - loss: 0.3032 - accuracy: 0.8768\n782/782 [==============================] - 24s 31ms/step - loss: 0.3351 - accuracy: 0.8620\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Compare the results","metadata":{}},{"cell_type":"code","source":"print(\"\\nResults on Short Sequences:\")\nprint(f\"Loss: {loss_short:.4f}, Accuracy: {accuracy_short:.4f}\")\n\nprint(\"\\nResults on Long Sequences:\")\nprint(f\"Loss: {loss_long:.4f}, Accuracy: {accuracy_long:.4f}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-04T03:42:16.899923Z","iopub.execute_input":"2023-08-04T03:42:16.900665Z","iopub.status.idle":"2023-08-04T03:42:16.906029Z","shell.execute_reply.started":"2023-08-04T03:42:16.900632Z","shell.execute_reply":"2023-08-04T03:42:16.905170Z"},"trusted":true},"execution_count":112,"outputs":[{"name":"stdout","text":"\nResults on Short Sequences:\nLoss: 0.3761, Accuracy: 0.8359\n\nResults on Long Sequences:\nLoss: 0.3351, Accuracy: 0.8620\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## with build_rnn_model_lstm\nResults on Short Sequences:\nLoss: 0.5788, Accuracy: 0.7310\n\nResults on Long Sequences:\nLoss: nan, Accuracy: 0.5000\n\n## with build_rnn_model_lstm and model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n\nResults on Short Sequences:\nLoss: 0.5542, Accuracy: 0.7568\n\nResults on Long Sequences:\nLoss: nan, Accuracy: 0.5000\n\n## with build_rnn_model_bi and model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\nResults on Short Sequences:\nLoss: 0.8635, Accuracy: 0.6448\n\nResults on Long Sequences:\nLoss: 0.4200, Accuracy: 0.8236\n## with build_rnn_model_bi and model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\nResults on Short Sequences:\nLoss: 0.3761, Accuracy: 0.8359\n\nResults on Long Sequences:\nLoss: 0.3351, Accuracy: 0.8620","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}