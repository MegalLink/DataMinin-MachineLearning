{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-22T21:12:38.041954Z","iopub.execute_input":"2023-08-22T21:12:38.043501Z","iopub.status.idle":"2023-08-22T21:12:38.052568Z","shell.execute_reply.started":"2023-08-22T21:12:38.043452Z","shell.execute_reply":"2023-08-22T21:12:38.050959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Long Short-Term Memory (LSTM) networks\nLSTM networks stand as a powerful solution to one of the most persistent challenges in training recurrent neural networks (RNNs): the vanishing gradient problem.\n\nIn traditional RNNs, the gradients can become very small as they propagate back through time, which impedes the networks' ability to capture long-range dependencies in sequential data. LSTMs address this issue with an ingenious architectural design that includes specialized memory cells and gating mechanisms. Each LSTM cell possesses the ability to remember or forget information over extended sequences, rendering them particularly adept at modeling sequences with extended gaps between relevant information.\n\nThe core of the LSTM architecture is its three gating mechanisms: the input gate, the forget gate, and the output gate. These gates allow LSTMs to determine what information to store, what information to discard, and how to update the cell's memory state. This unique design empowers LSTMs to effectively learn and maintain long-term dependencies in sequential data, making them a preferred choice for tasks such as machine translation, speech recognition, and sentiment analysis where capturing context over extended sequences is essential.\n\nExercise\nBuild an LSTM network to predict stock prices based on historical stock data. Show the model's ability to capture sequential dependencies.","metadata":{}},{"cell_type":"code","source":"!pip install yfinance","metadata":{"execution":{"iopub.status.busy":"2023-08-22T21:11:15.788606Z","iopub.execute_input":"2023-08-22T21:11:15.789105Z","iopub.status.idle":"2023-08-22T21:11:32.726254Z","shell.execute_reply.started":"2023-08-22T21:11:15.789070Z","shell.execute_reply":"2023-08-22T21:11:32.724628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import yfinance as yf\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense\nfrom sklearn.metrics import mean_squared_error","metadata":{"execution":{"iopub.status.busy":"2023-08-22T21:12:43.503012Z","iopub.execute_input":"2023-08-22T21:12:43.503475Z","iopub.status.idle":"2023-08-22T21:12:43.510025Z","shell.execute_reply.started":"2023-08-22T21:12:43.503440Z","shell.execute_reply":"2023-08-22T21:12:43.508550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fetch historical stock data using yfinance\nstock_symbol = \"AAPL\"\nstart_date = \"2020-01-01\"\nend_date = \"2023-01-01\"\nstock_data = yf.download(stock_symbol, start=start_date, end=end_date, progress=False)\n\n# Extract the 'Close' prices\nstock_prices = stock_data[\"Close\"].values\n# Reshape the data to 2D\nstock_prices = stock_prices.reshape(-1, 1)","metadata":{"execution":{"iopub.status.busy":"2023-08-22T21:12:46.693151Z","iopub.execute_input":"2023-08-22T21:12:46.693613Z","iopub.status.idle":"2023-08-22T21:12:46.888158Z","shell.execute_reply.started":"2023-08-22T21:12:46.693575Z","shell.execute_reply":"2023-08-22T21:12:46.887048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into training and testing sets\nsplit_date = len(stock_prices) - 120\ntrain_data = stock_prices[:split_date]\ntest_data = stock_prices[split_date:]\n","metadata":{"execution":{"iopub.status.busy":"2023-08-22T21:12:48.800088Z","iopub.execute_input":"2023-08-22T21:12:48.800635Z","iopub.status.idle":"2023-08-22T21:12:48.808711Z","shell.execute_reply.started":"2023-08-22T21:12:48.800589Z","shell.execute_reply":"2023-08-22T21:12:48.807718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model Architecture:\n\nCreate an LSTM model architecture. The architecture should include one or more LSTM layers, followed by one or more Dense layers for regression.\nExplain the concept of input sequences and time steps, as well as how to reshape the data to fit the LSTM input format.","metadata":{}},{"cell_type":"code","source":"# Create the LSTM model architecture\nmodel = Sequential([\n    LSTM(128, activation=\"relu\", input_shape=(None, 1), return_sequences=True),\n    LSTM(64, activation=\"relu\"),\n    Dense(1)\n])","metadata":{"execution":{"iopub.status.busy":"2023-08-22T21:12:50.356907Z","iopub.execute_input":"2023-08-22T21:12:50.357597Z","iopub.status.idle":"2023-08-22T21:12:50.584000Z","shell.execute_reply.started":"2023-08-22T21:12:50.357561Z","shell.execute_reply":"2023-08-22T21:12:50.581859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model Training:\n\nTrain the LSTM model using the training data. Explain the importance of setting appropriate hyperparameters, such as batch size and number of epochs.\nMonitor the training progress by plotting loss curves and observing how the model's performance changes over epochs.","metadata":{}},{"cell_type":"code","source":"# Set the hyperparameters\nbatch_size = 32\nepochs = 100\n\n# Train the model\nmodel.compile(loss=\"mse\", optimizer=\"adam\")\nmodel.fit(train_data, train_data, epochs=epochs, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-08-22T21:12:52.312675Z","iopub.execute_input":"2023-08-22T21:12:52.313085Z","iopub.status.idle":"2023-08-22T21:13:08.188815Z","shell.execute_reply.started":"2023-08-22T21:12:52.313054Z","shell.execute_reply":"2023-08-22T21:13:08.187944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on the testing data\npredictions = model.predict(test_data)","metadata":{"execution":{"iopub.status.busy":"2023-08-22T21:13:12.317124Z","iopub.execute_input":"2023-08-22T21:13:12.317618Z","iopub.status.idle":"2023-08-22T21:13:12.756397Z","shell.execute_reply.started":"2023-08-22T21:13:12.317579Z","shell.execute_reply":"2023-08-22T21:13:12.755114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model Evaluation:\n\nUse the trained model to make predictions on the testing data.\nEvaluate the model's performance using appropriate metrics like Mean Squared Error (MSE) or Root Mean Squared Error (RMSE).","metadata":{}},{"cell_type":"code","source":"# Evaluate the model's performance\nrmse = np.sqrt(mean_squared_error(predictions, test_data))\nprint(\"RMSE:\", rmse)","metadata":{"execution":{"iopub.status.busy":"2023-08-22T21:13:17.658759Z","iopub.execute_input":"2023-08-22T21:13:17.659190Z","iopub.status.idle":"2023-08-22T21:13:17.666987Z","shell.execute_reply.started":"2023-08-22T21:13:17.659158Z","shell.execute_reply":"2023-08-22T21:13:17.665891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lower RMSE: A lower RMSE value indicates that the predicted stock prices are closer to the true stock prices. In other words, a lower RMSE implies better predictive accuracy. A value close to 0 would mean the model's predictions are almost perfect.","metadata":{}},{"cell_type":"markdown","source":"Visualization:\n\nPlot the true stock prices and the predicted stock prices over time to visually assess the model's predictions.","metadata":{}},{"cell_type":"code","source":"# Plot the true stock prices and the predicted stock prices\nplt.figure(figsize=(10, 6))\nplt.plot(test_data, label=\"True Stock Prices\", color=\"red\") \nplt.plot(predictions, label=\"Predicted Stock Prices\", color=\"blue\")\nplt.title(\"True vs. Predicted Stock Prices\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Stock Price\")\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-22T21:13:22.967761Z","iopub.execute_input":"2023-08-22T21:13:22.968235Z","iopub.status.idle":"2023-08-22T21:13:23.344542Z","shell.execute_reply.started":"2023-08-22T21:13:22.968197Z","shell.execute_reply":"2023-08-22T21:13:23.343363Z"},"trusted":true},"execution_count":null,"outputs":[]}]}