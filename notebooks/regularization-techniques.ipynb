{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-04T05:28:54.417818Z","iopub.execute_input":"2023-08-04T05:28:54.418213Z","iopub.status.idle":"2023-08-04T05:28:54.425939Z","shell.execute_reply.started":"2023-08-04T05:28:54.418180Z","shell.execute_reply":"2023-08-04T05:28:54.424666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The primary reason to apply regularization techniques is to prevent overfitting in machine learning models, particularly in complex models like neural networks. Overfitting occurs when a model becomes too tailored to the training data, capturing noise and random fluctuations rather than general patterns. As a result, the model's performance on unseen data (i.e., the test data) degrades, leading to poor generalization.\n\nRegularization techniques aim to address overfitting by adding additional constraints to the model during training. These constraints encourage the model to focus on learning the essential patterns in the data while discouraging it from fitting noise or irrelevant details. By doing so, regularization improves the model's ability to generalize well to new, unseen data, which is the ultimate goal in machine learning.\n\nWeight Decay Regularization:\nWeight decay, also known as L2 regularization, is a technique used to prevent overfitting in neural networks by penalizing large weights. In weight decay, an additional term is added to the loss function that depends on the magnitude of the weights. The regularization term is proportional to the squared sum of all weights in the network.\n\nThe regularization term is defined as: Regularization_term = \n\nwhere \n is the regularization strength, and \n represents the weights of the network.\n\nThe role of weight decay in preventing overfitting is to discourage the model from relying too much on any particular feature or weight. By penalizing large weights, weight decay encourages the network to use smaller weights, resulting in a simpler model with reduced complexity. This, in turn, helps the model generalize better to unseen data and reduces the risk of overfitting.\n\nTo apply weight decay to a neural network during training, you need to add the regularization term to the loss function and adjust the learning process accordingly.\n\nDropout Regularization:\nDropout is a regularization technique that addresses overfitting by randomly deactivating (i.e., setting to zero) a fraction of neurons during training. In each training iteration, individual neurons are dropped with a certain probability, while during testing or inference, all neurons are active. This process effectively prevents complex co-adaptations among neurons, reducing the model's reliance on specific neurons and enhancing its generalization ability.\n\nThe role of dropout in preventing overfitting is to create an ensemble effect during training, where different subsets of neurons are activated or deactivated in each iteration. This ensemble approach leads to a more robust model that generalizes better to unseen data.\n\nTo apply dropout to a neural network is straightforward. You can add dropout layers after the activation functions in the hidden layers of your neural network.","metadata":{}},{"cell_type":"markdown","source":"Import libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-08-04T05:28:54.428210Z","iopub.execute_input":"2023-08-04T05:28:54.429011Z","iopub.status.idle":"2023-08-04T05:28:54.437847Z","shell.execute_reply.started":"2023-08-04T05:28:54.428978Z","shell.execute_reply":"2023-08-04T05:28:54.436564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load the CIFAR-10 dataset and preprocess the data.","metadata":{}},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-04T05:28:54.440058Z","iopub.execute_input":"2023-08-04T05:28:54.440591Z","iopub.status.idle":"2023-08-04T05:28:56.318083Z","shell.execute_reply.started":"2023-08-04T05:28:54.440537Z","shell.execute_reply":"2023-08-04T05:28:56.317126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define a neural network model with dropout layers.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(32 * 32 * 3, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 10)\n\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = x.view(-1, 32 * 32 * 3)\n        x = self.dropout(F.relu(self.fc1(x)))\n        x = self.dropout(F.relu(self.fc2(x)))\n        x = self.fc3(x)\n        return x\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-08-04T05:28:56.319309Z","iopub.execute_input":"2023-08-04T05:28:56.319607Z","iopub.status.idle":"2023-08-04T05:28:56.326999Z","shell.execute_reply.started":"2023-08-04T05:28:56.319581Z","shell.execute_reply":"2023-08-04T05:28:56.325992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-08-04T05:28:56.329382Z","iopub.execute_input":"2023-08-04T05:28:56.329705Z","iopub.status.idle":"2023-08-04T05:28:56.340298Z","shell.execute_reply.started":"2023-08-04T05:28:56.329678Z","shell.execute_reply":"2023-08-04T05:28:56.339106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 12, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(12, 32, 5)\n        self.fc1 = nn.Linear(32 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 32 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-08-04T05:28:56.341460Z","iopub.execute_input":"2023-08-04T05:28:56.341765Z","iopub.status.idle":"2023-08-04T05:28:56.351772Z","shell.execute_reply.started":"2023-08-04T05:28:56.341738Z","shell.execute_reply":"2023-08-04T05:28:56.350764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = Net()","metadata":{"execution":{"iopub.status.busy":"2023-08-04T05:28:56.353017Z","iopub.execute_input":"2023-08-04T05:28:56.353333Z","iopub.status.idle":"2023-08-04T05:28:56.367559Z","shell.execute_reply.started":"2023-08-04T05:28:56.353306Z","shell.execute_reply":"2023-08-04T05:28:56.366509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the loss function (cross-entropy) and optimizer (Stochastic Gradient Descent with weight decay).","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n#optimizer = optim.SGD(net.parameters(), lr=0.01, weight_decay=1e-4)\n#optimizer = optim.Adam(net.parameters(), lr=0.04, weight_decay=1e-3)\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-04T05:28:56.368914Z","iopub.execute_input":"2023-08-04T05:28:56.369275Z","iopub.status.idle":"2023-08-04T05:28:56.376532Z","shell.execute_reply.started":"2023-08-04T05:28:56.369245Z","shell.execute_reply":"2023-08-04T05:28:56.375643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train the model with weight decay and dropout for a certain number of epochs.","metadata":{}},{"cell_type":"code","source":"for epoch in range(5):  # Change the number of epochs as needed\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data\n\n        optimizer.zero_grad()\n\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if i % 200 == 199:  # Print every 200 mini-batches\n            print(f\"[{epoch + 1}, {i + 1}] Loss: {running_loss / 200:.3f}\")\n            running_loss = 0.0\n\nprint(\"Finished Training\")","metadata":{"execution":{"iopub.status.busy":"2023-08-04T05:28:56.378264Z","iopub.execute_input":"2023-08-04T05:28:56.379354Z","iopub.status.idle":"2023-08-04T05:31:15.667899Z","shell.execute_reply.started":"2023-08-04T05:28:56.379308Z","shell.execute_reply":"2023-08-04T05:31:15.666745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluate the model's accuracy on the test set.","metadata":{}},{"cell_type":"code","source":"correct = 0\ntotal = 0\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f\"Accuracy on the test set: {(100 * correct / total):.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-08-04T05:31:15.669533Z","iopub.execute_input":"2023-08-04T05:31:15.670737Z","iopub.status.idle":"2023-08-04T05:31:19.595581Z","shell.execute_reply.started":"2023-08-04T05:31:15.670693Z","shell.execute_reply":"2023-08-04T05:31:19.594270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NET 1\n### Net 1 with 5 epochs\noptim.SGD(net.parameters(), lr=0.01, weight_decay=1e-4)\nAccuracy on the test set: 45.23%\n### Net 1 with 5 epochs with adam \noptim.Adam(net.parameters(), lr=0.04, weight_decay=1e-3)\nAccuracy on the test set: 10.00%\n\n### Net 1 with 5 epochs \noptim.SGD(net.parameters(), lr=0.001, momentum=0.9)\nAccuracy on the test set: 44.31%\n\n## NET 2\n### Net 2 with 5 epochs\noptim.SGD(net.parameters(), lr=0.01, weight_decay=1e-4)\nAccuracy on the test set: 45.23%\n### Net 2 with 5 epochs with adam \noptim.Adam(net.parameters(), lr=0.04, weight_decay=1e-3)\nAccuracy on the test set: 10.00%\n\n### Net 2 with 5 epochs \noptim.SGD(net.parameters(), lr=0.001, momentum=0.9)\nAccuracy on the test set: 52.09%\n\n## NET 3\n### Net 3 with 5 epochs\noptim.SGD(net.parameters(), lr=0.01, weight_decay=1e-4)\nAccuracy on the test set: 53.91%\n\n### Net 3 with 5 epochs with adam \noptim.Adam(net.parameters(), lr=0.04, weight_decay=1e-3)\nAccuracy on the test set: 10.00%\n\n### Net 3 with 5 epochs \noptim.SGD(net.parameters(), lr=0.001, momentum=0.9)\nAccuracy on the test set: 55.13%\nAccuracy on the test set: 52.09%","metadata":{}},{"cell_type":"markdown","source":"Show an example of an image from the test dataset and its predicted class using the trained regularized model.","metadata":{}},{"cell_type":"code","source":"def imshow(img):\n    img = img / 2 + 0.5     # Unnormalize the image\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\nfor data in testloader:\n    images, labels = data\n    imshow(torchvision.utils.make_grid(images))  # Show the image\n    print('True Label: ', labels)\n\n    outputs = net(images)\n    _, predicted = torch.max(outputs, 1)\n    print('Predicted Label: ', predicted)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-08-04T05:31:19.599150Z","iopub.execute_input":"2023-08-04T05:31:19.599693Z","iopub.status.idle":"2023-08-04T05:31:19.973359Z","shell.execute_reply.started":"2023-08-04T05:31:19.599646Z","shell.execute_reply":"2023-08-04T05:31:19.972337Z"},"trusted":true},"execution_count":null,"outputs":[]}]}